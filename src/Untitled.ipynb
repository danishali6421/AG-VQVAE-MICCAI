{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fa2c42d-45ac-43c9-a508-a72bfcbb5878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a49cd7e9-5832-4e07-82a4-2316c4b1ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_schedule(time_step, N=100, att_1 = 0.99999, att_T = 0.000009, ctt_1 = 0.000009, ctt_T = 0.99999):\n",
    "    att = np.arange(0, time_step)/(time_step-1)*(att_T - att_1) + att_1\n",
    "    att = np.concatenate(([1], att))\n",
    "    at = att[1:]/att[:-1]\n",
    "    ctt = np.arange(0, time_step)/(time_step-1)*(ctt_T - ctt_1) + ctt_1\n",
    "    ctt = np.concatenate(([0], ctt))\n",
    "    one_minus_ctt = 1 - ctt\n",
    "    one_minus_ct = one_minus_ctt[1:] / one_minus_ctt[:-1]\n",
    "    ct = 1-one_minus_ct\n",
    "    bt = (1-at-ct)/N\n",
    "    att = np.concatenate((att[1:], [1]))\n",
    "    ctt = np.concatenate((ctt[1:], [0]))\n",
    "    btt = (1-att-ctt)/N\n",
    "    return at, bt, ct, att, btt, ctt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "272169d0-481f-46f8-b004-df1ba6e974a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "at, bt, ct, att, btt, ctt = alpha_schedule(3, N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44440976-db8b-4a3f-8e1f-ecffcb7d968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at [9.9999000e-01 5.0000450e-01 1.8000018e-05]\n",
      "bt [3.33333333e-07 1.66666667e-07 6.66654000e-07]\n",
      "ct [9.00000e-06 4.99995e-01 9.99980e-01]\n",
      "att [9.999900e-01 4.999995e-01 9.000000e-06 1.000000e+00]\n",
      "btt [3.33333333e-07 3.33333333e-07 3.33333333e-07 0.00000000e+00]\n",
      "att [9.000000e-06 4.999995e-01 9.999900e-01 0.000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"at\", at)\n",
    "print(\"bt\", bt)\n",
    "print(\"ct\", ct)\n",
    "print(\"att\", att)\n",
    "print(\"btt\", btt)\n",
    "print(\"att\", ctt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7151099-da26-4833-874a-179052e19c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "at = torch.tensor(at.astype('float64'))\n",
    "bt = torch.tensor(bt.astype('float64'))\n",
    "ct = torch.tensor(ct.astype('float64'))\n",
    "log_at = torch.log(at)\n",
    "log_bt = torch.log(bt)\n",
    "log_ct = torch.log(ct)\n",
    "att = torch.tensor(att.astype('float64'))\n",
    "btt = torch.tensor(btt.astype('float64'))\n",
    "ctt = torch.tensor(ctt.astype('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6778682-ab4a-40f1-aff6-bc9ea17ec16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at tensor([-1.0000e-05, -6.9314e-01, -1.0925e+01], dtype=torch.float64)\n",
      "bt tensor([-14.9141, -15.6073, -14.2210], dtype=torch.float64)\n",
      "ct tensor([-1.1618e+01, -6.9316e-01, -2.0000e-05], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"at\", log_at)\n",
    "print(\"bt\", log_bt)\n",
    "print(\"ct\", log_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39481b48-3b76-4b92-b825-fecd5f314c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_except_batch(x, num_dims=1):\n",
    "    return x.reshape(*x.shape[:num_dims], -1).sum(-1)\n",
    "\n",
    "def log_1_min_a(a):\n",
    "    return torch.log(1 - a.exp() + 1e-40)\n",
    "\n",
    "def log_add_exp(a, b):\n",
    "    maximum = torch.max(a, b)\n",
    "    return maximum + torch.log(torch.exp(a - maximum) + torch.exp(b - maximum))\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def log_categorical(log_x_start, log_prob):\n",
    "    return (log_x_start.exp() * log_prob).sum(dim=1)\n",
    "\n",
    "def index_to_log_onehot(x, num_classes):\n",
    "    assert x.max().item() < num_classes, \\\n",
    "        f'Error: {x.max().item()} >= {num_classes}'\n",
    "    x_onehot = F.one_hot(x, num_classes)\n",
    "    permute_order = (0, -1) + tuple(range(1, len(x.size())))\n",
    "    x_onehot = x_onehot.permute(permute_order)\n",
    "    log_x = torch.log(x_onehot.float().clamp(min=1e-30))\n",
    "    return log_x\n",
    "\n",
    "def log_onehot_to_index(log_x):\n",
    "    return log_x.argmax(1)\n",
    "\n",
    "def alpha_schedule(time_step, N=100, att_1 = 0.99999, att_T = 0.000009, ctt_1 = 0.000009, ctt_T = 0.99999):\n",
    "    att = np.arange(0, time_step)/(time_step-1)*(att_T - att_1) + att_1\n",
    "    att = np.concatenate(([1], att))\n",
    "    at = att[1:]/att[:-1]\n",
    "    ctt = np.arange(0, time_step)/(time_step-1)*(ctt_T - ctt_1) + ctt_1\n",
    "    ctt = np.concatenate(([0], ctt))\n",
    "    one_minus_ctt = 1 - ctt\n",
    "    one_minus_ct = one_minus_ctt[1:] / one_minus_ctt[:-1]\n",
    "    ct = 1-one_minus_ct\n",
    "    bt = (1-at-ct)/N\n",
    "    att = np.concatenate((att[1:], [1]))\n",
    "    ctt = np.concatenate((ctt[1:], [0]))\n",
    "    btt = (1-att-ctt)/N\n",
    "    return at, bt, ct, att, btt, ctt\n",
    "\n",
    "class DiffusionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        content_emb_config=None,\n",
    "        condition_emb_config=None,\n",
    "        transformer_config=None,\n",
    "\n",
    "        diffusion_step=100,\n",
    "        alpha_init_type='cos',\n",
    "        auxiliary_loss_weight=0,\n",
    "        adaptive_auxiliary_loss=False,\n",
    "        mask_weight=[1,1],\n",
    "\n",
    "        learnable_cf=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if condition_emb_config is None:\n",
    "            self.condition_emb = None\n",
    "        else:\n",
    "            # for condition and config, we learn a seperate embedding\n",
    "            self.condition_emb = None\n",
    "            self.condition_dim = None\n",
    "       \n",
    "        transformer_config= diffusion_step\n",
    "        transformer_config = content_emb_config\n",
    "        self.transformer = None\n",
    "        self.content_seq_len = transformer_config\n",
    "        self.amp = False\n",
    "\n",
    "        self.num_classes = None\n",
    "        self.loss_type = 'vb_stochastic'\n",
    "        self.shape = transformer_config\n",
    "        self.num_timesteps = diffusion_step\n",
    "        self.parametrization = 'x0'\n",
    "        self.auxiliary_loss_weight = auxiliary_loss_weight\n",
    "        self.adaptive_auxiliary_loss = adaptive_auxiliary_loss\n",
    "        self.mask_weight = mask_weight\n",
    "\n",
    "        if alpha_init_type == \"alpha1\":\n",
    "            at, bt, ct, att, btt, ctt = alpha_schedule(5, N=3)\n",
    "        else:\n",
    "            print(\"alpha_init_type is Wrong !! \")\n",
    "\n",
    "        at = torch.tensor(at.astype('float64'))\n",
    "        bt = torch.tensor(bt.astype('float64'))\n",
    "        ct = torch.tensor(ct.astype('float64'))\n",
    "        log_at = torch.log(at)\n",
    "        log_bt = torch.log(bt)\n",
    "        log_ct = torch.log(ct)\n",
    "        att = torch.tensor(att.astype('float64'))\n",
    "        btt = torch.tensor(btt.astype('float64'))\n",
    "        ctt = torch.tensor(ctt.astype('float64'))\n",
    "        log_cumprod_at = torch.log(att)\n",
    "        log_cumprod_bt = torch.log(btt)\n",
    "        log_cumprod_ct = torch.log(ctt)\n",
    "\n",
    "        log_1_min_ct = log_1_min_a(log_ct)\n",
    "        log_1_min_cumprod_ct = log_1_min_a(log_cumprod_ct)\n",
    "\n",
    "        assert log_add_exp(log_ct, log_1_min_ct).abs().sum().item() < 1.e-5\n",
    "        assert log_add_exp(log_cumprod_ct, log_1_min_cumprod_ct).abs().sum().item() < 1.e-5\n",
    "\n",
    "        self.diffusion_acc_list = [0] * self.num_timesteps\n",
    "        self.diffusion_keep_list = [0] * self.num_timesteps\n",
    "        # Convert to float32 and register buffers.\n",
    "        self.register_buffer('log_at', log_at.float())\n",
    "        self.register_buffer('log_bt', log_bt.float())\n",
    "        self.register_buffer('log_ct', log_ct.float())\n",
    "        self.register_buffer('log_cumprod_at', log_cumprod_at.float())\n",
    "        self.register_buffer('log_cumprod_bt', log_cumprod_bt.float())\n",
    "        self.register_buffer('log_cumprod_ct', log_cumprod_ct.float())\n",
    "        self.register_buffer('log_1_min_ct', log_1_min_ct.float())\n",
    "        self.register_buffer('log_1_min_cumprod_ct', log_1_min_cumprod_ct.float())\n",
    "\n",
    "        self.register_buffer('Lt_history', torch.zeros(self.num_timesteps))\n",
    "        self.register_buffer('Lt_count', torch.zeros(self.num_timesteps))\n",
    "        self.zero_vector = None\n",
    "\n",
    "        if learnable_cf:\n",
    "            self.empty_text_embed = torch.nn.Parameter(torch.randn(size=(77, 512), requires_grad=True, dtype=torch.float64))\n",
    "\n",
    "        self.prior_rule = 0    # inference rule: 0 for VQ-Diffusion v1, 1 for only high-quality inference, 2 for purity prior\n",
    "        self.prior_ps = 1024   # max number to sample per step\n",
    "        self.prior_weight = 0  # probability adjust parameter, 'r' in Equation.11 of Improved VQ-Diffusion\n",
    "\n",
    "        self.update_n_sample()\n",
    "        \n",
    "        self.learnable_cf = learnable_cf\n",
    "    \n",
    "\n",
    "    def update_n_sample(self):\n",
    "        if self.num_timesteps == 100:\n",
    "            if self.prior_ps <= 10:\n",
    "                self.n_sample = [1, 6] + [11, 10, 10] * 32 + [11, 15]\n",
    "            else:\n",
    "                self.n_sample = [1, 10] + [11, 10, 10] * 32 + [11, 11]\n",
    "        elif self.num_timesteps == 50:\n",
    "            self.n_sample = [10] + [21, 20] * 24 + [30]\n",
    "        elif self.num_timesteps == 25:\n",
    "            self.n_sample = [21] + [41] * 23 + [60]\n",
    "        elif self.num_timesteps == 10:\n",
    "            self.n_sample = [69] + [102] * 8 + [139]\n",
    "\n",
    "    def multinomial_kl(self, log_prob1, log_prob2):   # compute KL loss on log_prob\n",
    "        kl = (log_prob1.exp() * (log_prob1 - log_prob2)).sum(dim=1)\n",
    "        return kl\n",
    "\n",
    "    def q_pred_one_timestep(self, log_x_t, t):         # q(xt|xt_1)\n",
    "        log_at = extract(self.log_at, t, log_x_t.shape)             # at\n",
    "        log_bt = extract(self.log_bt, t, log_x_t.shape)             # bt\n",
    "        log_ct = extract(self.log_ct, t, log_x_t.shape)             # ct\n",
    "        log_1_min_ct = extract(self.log_1_min_ct, t, log_x_t.shape)          # 1-ct\n",
    "\n",
    "        log_probs = torch.cat(\n",
    "            [\n",
    "                log_add_exp(log_x_t[:,:-1,:]+log_at, log_bt),\n",
    "                log_add_exp(log_x_t[:, -1:, :] + log_1_min_ct, log_ct)\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def q_pred(self, log_x_start, t):           # q(xt|x0)\n",
    "        # log_x_start can be onehot or not\n",
    "        t = (t + (self.num_timesteps + 1))%(self.num_timesteps + 1)\n",
    "        log_cumprod_at = extract(self.log_cumprod_at, t, log_x_start.shape)         # at~\n",
    "        log_cumprod_bt = extract(self.log_cumprod_bt, t, log_x_start.shape)         # bt~\n",
    "        log_cumprod_ct = extract(self.log_cumprod_ct, t, log_x_start.shape)         # ct~\n",
    "        log_1_min_cumprod_ct = extract(self.log_1_min_cumprod_ct, t, log_x_start.shape)       # 1-ct~\n",
    "        \n",
    "\n",
    "        log_probs = torch.cat(\n",
    "            [\n",
    "                log_add_exp(log_x_start[:,:-1,:]+log_cumprod_at, log_cumprod_bt),\n",
    "                log_add_exp(log_x_start[:,-1:,:]+log_1_min_cumprod_ct, log_cumprod_ct)\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def predict_start(self, log_x_t, cond_emb, t):          # p(x0|xt)\n",
    "        x_t = log_onehot_to_index(log_x_t)\n",
    "        if self.amp == True:\n",
    "            with autocast():\n",
    "                out = self.transformer(x_t, cond_emb, t)\n",
    "        else:\n",
    "            out = self.transformer(x_t, cond_emb, t)\n",
    "\n",
    "        assert out.size(0) == x_t.size(0)\n",
    "        assert out.size(1) == self.num_classes-1\n",
    "        assert out.size()[2:] == x_t.size()[1:]\n",
    "        log_pred = F.log_softmax(out.double(), dim=1).float()\n",
    "        batch_size = log_x_t.size()[0]\n",
    "        if self.zero_vector is None or self.zero_vector.shape[0] != batch_size:\n",
    "            self.zero_vector = torch.zeros(batch_size, 1, self.content_seq_len).type_as(log_x_t)- 70\n",
    "        log_pred = torch.cat((log_pred, self.zero_vector), dim=1)\n",
    "        log_pred = torch.clamp(log_pred, -70, 0)\n",
    "\n",
    "        return log_pred\n",
    "    \n",
    "    def cf_predict_start(self, log_x_t, cond_emb, t):\n",
    "        return self.predict_start(log_x_t, cond_emb, t)\n",
    "\n",
    "    def q_posterior(self, log_x_start, log_x_t, t):            # p_theta(xt_1|xt) = sum(q(xt-1|xt,x0')*p(x0'))\n",
    "        # notice that log_x_t is onehot\n",
    "        assert t.min().item() >= 0 and t.max().item() < self.num_timesteps\n",
    "        batch_size = log_x_start.size()[0]\n",
    "        onehot_x_t = log_onehot_to_index(log_x_t)\n",
    "        mask = (onehot_x_t == self.num_classes-1).unsqueeze(1) \n",
    "        log_one_vector = torch.zeros(batch_size, 1, 1).type_as(log_x_t)\n",
    "        log_zero_vector = torch.log(log_one_vector+1.0e-30).expand(-1, -1, self.content_seq_len)\n",
    "\n",
    "        log_qt = self.q_pred(log_x_t, t)                                  # q(xt|x0)\n",
    "        # log_qt = torch.cat((log_qt[:,:-1,:], log_zero_vector), dim=1)\n",
    "        log_qt = log_qt[:,:-1,:]\n",
    "        log_cumprod_ct = extract(self.log_cumprod_ct, t, log_x_start.shape)         # ct~\n",
    "        ct_cumprod_vector = log_cumprod_ct.expand(-1, self.num_classes-1, -1)\n",
    "        # ct_cumprod_vector = torch.cat((ct_cumprod_vector, log_one_vector), dim=1)\n",
    "        log_qt = (~mask)*log_qt + mask*ct_cumprod_vector\n",
    "        \n",
    "\n",
    "        log_qt_one_timestep = self.q_pred_one_timestep(log_x_t, t)        # q(xt|xt_1)\n",
    "        log_qt_one_timestep = torch.cat((log_qt_one_timestep[:,:-1,:], log_zero_vector), dim=1)\n",
    "        log_ct = extract(self.log_ct, t, log_x_start.shape)         # ct\n",
    "        ct_vector = log_ct.expand(-1, self.num_classes-1, -1)\n",
    "        ct_vector = torch.cat((ct_vector, log_one_vector), dim=1)\n",
    "        log_qt_one_timestep = (~mask)*log_qt_one_timestep + mask*ct_vector\n",
    "        \n",
    "        # log_x_start = torch.cat((log_x_start, log_zero_vector), dim=1)\n",
    "        # q = log_x_start - log_qt\n",
    "        q = log_x_start[:,:-1,:] - log_qt\n",
    "        q = torch.cat((q, log_zero_vector), dim=1)\n",
    "        q_log_sum_exp = torch.logsumexp(q, dim=1, keepdim=True)\n",
    "        q = q - q_log_sum_exp\n",
    "        log_EV_xtmin_given_xt_given_xstart = self.q_pred(q, t-1) + log_qt_one_timestep + q_log_sum_exp\n",
    "        return torch.clamp(log_EV_xtmin_given_xt_given_xstart, -70, 0)\n",
    "\n",
    "    def p_pred(self, log_x, cond_emb, t):             # if x0, first p(x0|xt), than sum(q(xt-1|xt,x0)*p(x0|xt))\n",
    "        if self.parametrization == 'x0':\n",
    "            log_x_recon = self.cf_predict_start(log_x, cond_emb, t)\n",
    "            log_model_pred = self.q_posterior(\n",
    "                log_x_start=log_x_recon, log_x_t=log_x, t=t)\n",
    "        elif self.parametrization == 'direct':\n",
    "            log_model_pred = self.predict_start(log_x, cond_emb, t)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        return log_model_pred, log_x_recon\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, log_x, cond_emb, t, sampled=None, to_sample=None):               # sample q(xt-1) for next step from  xt, actually is p(xt-1|xt)\n",
    "        model_log_prob, log_x_recon = self.p_pred(log_x, cond_emb, t)\n",
    "\n",
    "        max_sample_per_step = self.prior_ps  # max number to sample per step\n",
    "        if t[0] > 0 and self.prior_rule > 0 and to_sample is not None: # prior_rule: 0 for VQ-Diffusion v1, 1 for only high-quality inference, 2 for purity prior\n",
    "            log_x_idx = log_onehot_to_index(log_x)\n",
    "\n",
    "            if self.prior_rule == 1:\n",
    "                score = torch.ones((log_x.shape[0], log_x.shape[2])).to(log_x.device)\n",
    "            elif self.prior_rule == 2:\n",
    "                score = torch.exp(log_x_recon).max(dim=1).values.clamp(0, 1)\n",
    "                score /= (score.max(dim=1, keepdim=True).values + 1e-10)\n",
    "\n",
    "            if self.prior_rule != 1 and self.prior_weight > 0:\n",
    "                # probability adjust parameter, prior_weight: 'r' in Equation.11 of Improved VQ-Diffusion\n",
    "                prob = ((1 + score * self.prior_weight).unsqueeze(1) * log_x_recon).softmax(dim=1)\n",
    "                prob = prob.log().clamp(-70, 0)\n",
    "            else:\n",
    "                prob = log_x_recon\n",
    "\n",
    "            out = self.log_sample_categorical(prob)\n",
    "            out_idx = log_onehot_to_index(out)\n",
    "\n",
    "            out2_idx = log_x_idx.clone()\n",
    "            _score = score.clone()\n",
    "            if _score.sum() < 1e-6:\n",
    "                _score += 1\n",
    "            _score[log_x_idx != self.num_classes - 1] = 0\n",
    "\n",
    "            for i in range(log_x.shape[0]):\n",
    "                n_sample = min(to_sample - sampled[i], max_sample_per_step)\n",
    "                if to_sample - sampled[i] - n_sample == 1:\n",
    "                    n_sample = to_sample - sampled[i]\n",
    "                if n_sample <= 0:\n",
    "                    continue\n",
    "                sel = torch.multinomial(_score[i], n_sample)\n",
    "                out2_idx[i][sel] = out_idx[i][sel]\n",
    "                sampled[i] += ((out2_idx[i] != self.num_classes - 1).sum() - (log_x_idx[i] != self.num_classes - 1).sum()).item()\n",
    "\n",
    "            out = index_to_log_onehot(out2_idx, self.num_classes)\n",
    "        else:\n",
    "            # Gumbel sample\n",
    "            out = self.log_sample_categorical(model_log_prob)\n",
    "            sampled = [1024] * log_x.shape[0]\n",
    "\n",
    "        if to_sample is not None:\n",
    "            return out, sampled\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def log_sample_categorical(self, logits):           # use gumbel to sample onehot vector from log probability\n",
    "        uniform = torch.rand_like(logits)\n",
    "        gumbel_noise = -torch.log(-torch.log(uniform + 1e-30) + 1e-30)\n",
    "        sample = (gumbel_noise + logits).argmax(dim=1)\n",
    "        log_sample = index_to_log_onehot(sample, self.num_classes)\n",
    "        return log_sample\n",
    "\n",
    "    def q_sample(self, log_x_start, t):                 # diffusion step, q(xt|x0) and sample xt\n",
    "        log_EV_qxt_x0 = self.q_pred(log_x_start, t)\n",
    "\n",
    "        log_sample = self.log_sample_categorical(log_EV_qxt_x0)\n",
    "\n",
    "        return log_sample\n",
    "\n",
    "    def sample_time(self, b, device, method='uniform'):\n",
    "        if method == 'importance':\n",
    "            if not (self.Lt_count > 10).all():\n",
    "                return self.sample_time(b, device, method='uniform')\n",
    "\n",
    "            Lt_sqrt = torch.sqrt(self.Lt_history + 1e-10) + 0.0001\n",
    "            Lt_sqrt[0] = Lt_sqrt[1]  # Overwrite decoder term with L1.\n",
    "            pt_all = Lt_sqrt / Lt_sqrt.sum()\n",
    "\n",
    "            t = torch.multinomial(pt_all, num_samples=b, replacement=True)\n",
    "\n",
    "            pt = pt_all.gather(dim=0, index=t)\n",
    "\n",
    "            return t, pt\n",
    "\n",
    "        elif method == 'uniform':\n",
    "            t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "\n",
    "            pt = torch.ones_like(t).float() / self.num_timesteps\n",
    "            return t, pt\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def _train_loss(self, x, cond_emb, is_train=True):                       # get the KL loss\n",
    "        b, device = x.size(0), x.device\n",
    "\n",
    "        assert self.loss_type == 'vb_stochastic'\n",
    "        x_start = x\n",
    "        t, pt = self.sample_time(b, device, 'importance')\n",
    "\n",
    "\n",
    "        log_x_start = index_to_log_onehot(x_start, self.num_classes)\n",
    "        log_xt = self.q_sample(log_x_start=log_x_start, t=t)\n",
    "        xt = log_onehot_to_index(log_xt)\n",
    "\n",
    "        ############### go to p_theta function ###############\n",
    "        log_x0_recon = self.predict_start(log_xt, cond_emb, t=t)            # P_theta(x0|xt)\n",
    "        log_model_prob = self.q_posterior(log_x_start=log_x0_recon, log_x_t=log_xt, t=t)      # go through q(xt_1|xt,x0)\n",
    "\n",
    "        ################## compute acc list ################\n",
    "        x0_recon = log_onehot_to_index(log_x0_recon)\n",
    "        x0_real = x_start\n",
    "        xt_1_recon = log_onehot_to_index(log_model_prob)\n",
    "        xt_recon = log_onehot_to_index(log_xt)\n",
    "        for index in range(t.size()[0]):\n",
    "            this_t = t[index].item()\n",
    "            same_rate = (x0_recon[index] == x0_real[index]).sum().cpu()/x0_real.size()[1]\n",
    "            self.diffusion_acc_list[this_t] = same_rate.item()*0.1 + self.diffusion_acc_list[this_t]*0.9\n",
    "            same_rate = (xt_1_recon[index] == xt_recon[index]).sum().cpu()/xt_recon.size()[1]\n",
    "            self.diffusion_keep_list[this_t] = same_rate.item()*0.1 + self.diffusion_keep_list[this_t]*0.9\n",
    "\n",
    "        # compute log_true_prob now \n",
    "        log_true_prob = self.q_posterior(log_x_start=log_x_start, log_x_t=log_xt, t=t)\n",
    "        kl = self.multinomial_kl(log_true_prob, log_model_prob)\n",
    "        mask_region = (xt == self.num_classes-1).float()\n",
    "        mask_weight = mask_region * self.mask_weight[0] + (1. - mask_region) * self.mask_weight[1]\n",
    "        kl = kl * mask_weight\n",
    "        kl = sum_except_batch(kl)\n",
    "\n",
    "        decoder_nll = -log_categorical(log_x_start, log_model_prob)\n",
    "        decoder_nll = sum_except_batch(decoder_nll)\n",
    "\n",
    "        mask = (t == torch.zeros_like(t)).float()\n",
    "        kl_loss = mask * decoder_nll + (1. - mask) * kl\n",
    "        \n",
    "\n",
    "        Lt2 = kl_loss.pow(2)\n",
    "        Lt2_prev = self.Lt_history.gather(dim=0, index=t)\n",
    "        new_Lt_history = (0.1 * Lt2 + 0.9 * Lt2_prev).detach()\n",
    "        self.Lt_history.scatter_(dim=0, index=t, src=new_Lt_history)\n",
    "        self.Lt_count.scatter_add_(dim=0, index=t, src=torch.ones_like(Lt2))\n",
    "\n",
    "        # Upweigh loss term of the kl\n",
    "        # vb_loss = kl_loss / pt + kl_prior\n",
    "        loss1 = kl_loss / pt \n",
    "        vb_loss = loss1\n",
    "        if self.auxiliary_loss_weight != 0 and is_train==True:\n",
    "            kl_aux = self.multinomial_kl(log_x_start[:,:-1,:], log_x0_recon[:,:-1,:])\n",
    "            kl_aux = kl_aux * mask_weight\n",
    "            kl_aux = sum_except_batch(kl_aux)\n",
    "            kl_aux_loss = mask * decoder_nll + (1. - mask) * kl_aux\n",
    "            if self.adaptive_auxiliary_loss == True:\n",
    "                addition_loss_weight = (1-t/self.num_timesteps) + 1.0\n",
    "            else:\n",
    "                addition_loss_weight = 1.0\n",
    "\n",
    "            loss2 = addition_loss_weight * self.auxiliary_loss_weight * kl_aux_loss / pt\n",
    "            vb_loss += loss2\n",
    "\n",
    "        return log_model_prob, vb_loss\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.transformer.to_logits[-1].weight.device\n",
    "\n",
    "    def parameters(self, recurse=True, name=None):\n",
    "        \"\"\"\n",
    "        Following minGPT:\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "        # return super().parameters(recurse=True)\n",
    "        if name is None or name == 'none':\n",
    "            return super().parameters(recurse=recurse)\n",
    "        else:\n",
    "            # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "            print(\"GPTLikeTransformer: get parameters by the overwrite method!\")\n",
    "            decay = set()\n",
    "            no_decay = set()\n",
    "            whitelist_weight_modules = (torch.nn.Linear, )\n",
    "            blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "            for mn, m in self.named_modules():\n",
    "                for pn, p in m.named_parameters():\n",
    "                    fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                    if pn.endswith('bias'):\n",
    "                        # all biases will not be decayed\n",
    "                        no_decay.add(fpn)\n",
    "                    elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                        # weights of whitelist modules will be weight decayed\n",
    "                        decay.add(fpn)\n",
    "                    elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                        # weights of blacklist modules will NOT be weight decayed\n",
    "                        no_decay.add(fpn)\n",
    "            # special case the position embedding parameter as not decayed\n",
    "            module_name = ['condition_emb', 'content_emb']\n",
    "            pos_emb_name = ['pos_emb', 'width_emb', 'height_emb', 'pad_emb', 'token_type_emb']\n",
    "            for mn in module_name:\n",
    "                if hasattr(self, mn) and getattr(self, mn) is not None:\n",
    "                    for pn in pos_emb_name:\n",
    "                        if hasattr(getattr(self, mn), pn):\n",
    "                            if isinstance(getattr(getattr(self, mn), pn), torch.nn.Parameter):\n",
    "                                no_decay.add('{}.{}'.format(mn, pn))\n",
    "\n",
    "            # validate that we considered every parameter\n",
    "            param_dict = {pn: p for pn, p in self.transformer.named_parameters()}# if p.requires_grad} \n",
    "            inter_params = decay & no_decay\n",
    "            union_params = decay | no_decay\n",
    "            assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "            assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                        % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "            # create the pytorch optimizer object\n",
    "            optim_groups = [\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": 0.01},\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "            ]\n",
    "            return optim_groups\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input, \n",
    "            return_loss=False, \n",
    "            return_logits=True, \n",
    "            return_att_weight=False,\n",
    "            is_train=True,\n",
    "            **kwargs):\n",
    "        if kwargs.get('autocast') == True:\n",
    "            self.amp = True\n",
    "        batch_size = input['content_token'].shape[0]\n",
    "        device = input['content_token'].device\n",
    "\n",
    "        # 1) get embeddding for condition and content     prepare input\n",
    "        sample_image = input['content_token'].type_as(input['content_token'])\n",
    "        # cont_emb = self.content_emb(sample_image)\n",
    "\n",
    "        if self.condition_emb is not None:\n",
    "            with autocast(enabled=False):\n",
    "                with torch.no_grad():\n",
    "                    cond_emb = self.condition_emb(input['condition_token']) # B x Ld x D   #256*1024\n",
    "                if self.learnable_cf:\n",
    "                    is_empty_text = torch.logical_not(input['condition_mask'][:, 2]).unsqueeze(1).unsqueeze(2).repeat(1, 77, 512)\n",
    "                    cond_emb = torch.where(is_empty_text, self.empty_text_embed.unsqueeze(0).repeat(cond_emb.shape[0], 1, 1), cond_emb.type_as(self.empty_text_embed))\n",
    "                cond_emb = cond_emb.float()\n",
    "        else: # share condition embeding with content\n",
    "            if input.get('condition_embed_token') == None:\n",
    "                cond_emb = None\n",
    "            else:\n",
    "                cond_emb = input['condition_embed_token'].float()\n",
    "            \n",
    "        # now we get cond_emb and sample_image\n",
    "        if is_train == True:\n",
    "            log_model_prob, loss = self._train_loss(sample_image, cond_emb)\n",
    "            loss = loss.sum()/(sample_image.size()[0] * sample_image.size()[1])\n",
    "\n",
    "        # 4) get output, especially loss\n",
    "        out = {}\n",
    "        if return_logits:\n",
    "            out['logits'] = torch.exp(log_model_prob)\n",
    "\n",
    "        if return_loss:\n",
    "            out['loss'] = loss \n",
    "        self.amp = False\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45966349-de45-4362-ba6a-d100d04ec580",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test=DiffusionTransformer(alpha_init_type='alpha1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40bb449f-f4f2-450c-a0ba-d0b7c206b8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "x_start=torch.rand(1, 5)\n",
    "x_start=torch.rand_like(x_start).long()\n",
    "print(x_start.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f759f10-c384-4816-b6d3-56f30b61fda1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmy_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_start\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/custombts/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/custombts/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[55], line 485\u001b[0m, in \u001b[0;36mDiffusionTransformer.forward\u001b[0;34m(self, input, return_loss, return_logits, return_att_weight, is_train, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautocast\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent_token\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    486\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_token\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# 1) get embeddding for condition and content     prepare input\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "my_test(x_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70a5673b-eea1-49f2-8e46-85b8da5f87a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "q_sample() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m log_x_start \u001b[38;5;241m=\u001b[39m index_to_log_onehot(x_start, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m log_xt \u001b[38;5;241m=\u001b[39m \u001b[43mq_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_x_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_x_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m xt \u001b[38;5;241m=\u001b[39m log_onehot_to_index(log_xt)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m############### go to p_theta function ###############\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: q_sample() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "log_x_start = index_to_log_onehot(x_start, 5)\n",
    "log_xt = q_sample(log_x_start=log_x_start, t=1)\n",
    "xt = log_onehot_to_index(log_xt)\n",
    "\n",
    "############### go to p_theta function ###############\n",
    "log_x0_recon = predict_start(log_xt, cond_emb, t=t)            # P_theta(0)\n",
    "log_model_prob = q_posterior(log_x_start=log_x0_recon, log_x_t=log_xt, t=t)      #q(xt_1|xt,x0)\n",
    "\n",
    "################## compute acc list ################\n",
    "x0_recon = log_onehot_to_index(log_x0_recon)\n",
    "x0_real = x_start\n",
    "xt_1_recon = log_onehot_to_index(log_model_prob)\n",
    "xt_recon = log_onehot_to_index(log_xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd2852-d629-4082-a236-64a1e7f2f9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56a7f2-feab-4b96-ad1f-37c460455bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
